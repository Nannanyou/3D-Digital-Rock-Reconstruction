D = {'func': 'networks.D_paper'}
D_loss = {'func': 'loss.D_wgangp_acgan'}
D_opt = {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08}
EasyDict = <class 'config.EasyDict'>
G = {'func': 'networks.G_paper', 'latent_size': 512, 'fused_scale': True, 'up_scale_method': 'nearest', 'normalize_latents': True}
G_loss = {'func': 'loss.G_wgan_acgan'}
G_opt = {'beta1': 0.0, 'beta2': 0.99, 'epsilon': 1e-08}
data_dir = datasets
dataset = {'tfrecord_dir': 'carbonate_case11_extended'}
desc = pgan-latent512-fusedup_convk2_1conv-1conv_fuseddown_convk3-case11_extended-normLatent_T-preset-v2-8gpus-HIST
env = {'TF_CPP_MIN_LOG_LEVEL': '1'}
grid = {'size': '1080p', 'layout': 'random'}
num_gpus = 8
random_seed = 1000
result_dir = results_new
sched = {'minibatch_base': 32, 'minibatch_dict': {4: 512, 8: 256, 16: 128, 32: 64, 64: 32}, 'G_lrate_dict': {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}, 'D_lrate_dict': {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}, 'lod_training_kimg': 600, 'lod_transition_kimg': 600}
tf_config = {'graph_options.place_pruned_graph': True}
train = {'func': 'train.train_progressive_gan', 'mirror_augment': True, 'total_kimg': 12000, 'save_weight_histograms': True}
